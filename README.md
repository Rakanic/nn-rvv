# nn-rvv

**nn-rvv** is a lightweight deepâ€‘learning framework for compiling highâ€‘level neural networks into efficient, bareâ€‘metal C code targeting the **RISCâ€‘V Vector ExtensionÂ (RVV)Â 1.0**. Built for and tested within the Chipyard environment, it currently targets the **Saturn Vector Unit**.

âœ…Â **Quantization Support** â€“ Full symmetric *and* asymmetric flows, including zeroâ€‘points, bias quantization and perâ€‘layer clamping  
âš ï¸Â **Roadmap Focus** â€“ Ongoing work on a smoother Pythonâ€¯â†’â€¯C compilation path and deeper RVV kernel optimizations (3â€¯Ã—â€¯3 depthwise first, then 5â€¯Ã—â€¯5 and 1â€¯Ã—â€¯1 pointwise)

---

## ğŸš€ Features

| Layer / Op                       | f32 | int8 |
|----------------------------------|:---:|:----:|
| FullyÂ Connected                  | âœ…  | âœ…   |
| Depthwiseâ€¯Conv2DÂ (3â€¯Ã—â€¯3)          | âœ…  | âœ…   |
| Depthwiseâ€¯Conv2DÂ (5â€¯Ã—â€¯5)          | âœ…  | âœ…   |
| Pointwiseâ€¯ConvÂ (1â€¯Ã—â€¯1)            | âœ…  | âœ…   |
| MaxÂ Pool                         | âœ…  | âœ…   |
| Softmax                          | âœ…  | âŒ   |
| Transpose                        | âœ…  | âœ…   |
| Quantizeâ€¯/â€¯Dequantize            |Â â€”Â   | âœ…   |
| MultithreadingÂ (basic)           | âœ…  | âœ…   |

---

## ğŸ”§ Getting Started

1. **Prerequisites**  
   â€¢Â A RISCâ€‘V toolchain with **RVVÂ 1.0** support  
   â€¢Â A Chipyard installation with the appropriate conda environment sourced (Spike, `riscv64-unknown-elf-gcc`, â€¦)

---

## ğŸ§  Example Network

```python
class DWPWNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.dw0   = nn.Conv2d(1, 1, 3, groups=1, bias=False)
        self.pw0   = nn.Conv2d(1, 16, 1, bias=True)
        self.pool0 = nn.MaxPool2d(3, 3)
        self.dw1   = nn.Conv2d(16, 16, 3, groups=16, bias=False)
        self.pw1   = nn.Conv2d(16, 32, 1, bias=True)
        self.pool1 = nn.MaxPool2d(3, 3)
        self.fc0   = nn.Linear(32*2*2, 32)
        self.fc1   = nn.Linear(32, 10)
```

Youâ€™ll find the full file at `models/mnist_models/mnist_cnn/mnist_cnn.py`.

---

## ğŸ§ª TrainingÂ + Compilation Workflow

**StepÂ 1Â â€“Â TrainÂ (float32)**  
```bash
cd models/mnist_models/mnist_cnn
python mnist_cnn.py      # dumps model_params.h with learned weights
```

**StepÂ 2Â â€“Â CompileÂ & RunÂ (float32 inference)**  
```bash
make mnist_cnn
spike --isa=rv64gcv_zicntr mnist_cnn.riscv
```

These examples are using spike, but you can also run the binary in RTL simulation in Chipyard using a Saturn configuration. 
---

## ğŸ§® Quantized Workflow

Quantized inference is fully supported and stable. This example uses a standard symmetric flow and I didn't include bias in the layers, but asymmetric with zeroâ€‘points and int32 biases is also available.

```python
# Example forward snippet
x = torch.clamp(x * self.input_scale, min=-128, max=127).round()
x = self.dw0(x)
x = torch.clamp(x * self.dw0.output_scale, min=-128, max=127).round()
# ...
x = self.fc1(x)
x = torch.clamp(x * self.fc1.output_scale, min=-128, max=127).round()
```

**StepÂ 1Â â€“Â QuantizeÂ & Export**  
Run `export_quantized_to_header.py` to generate `model_params.h` (weights + scales).

**StepÂ 2Â â€“Â CompileÂ & RunÂ (int8 inference)**  
```bash
make mnist_cnn_quant
spike --isa=rv64gcv_zicntr mnist_cnn_quant.riscv
```

---

## ğŸ§µ Multicore Support

Basic multicore inference targets are provided:

```bash
make mnist_cnn_mc
spike -p2 --isa=rv64gcv_zicntr mnist_cnn_mc.riscv   # run on 2 cores
```

---

## ğŸ“‚ Example Models

```bash
make mnist
make mnist_cnn
make mnist_cnn_mc
make mnist_quant2
make mnist_quant2_mc
make mnist_cnn_quant
make mnist_quant
```

Run with:
```bash
spike --isa=rv64gcv_zicntr <binary>.riscv
```

---

## ğŸ§ª Sample Output

You should ideally see something like this 
```
SampleÂ 0Â â†’Â 7   probs:Â 0Â 0Â 0Â 0Â 0Â 0Â 0Â 100Â 0Â 0
SampleÂ 1Â â†’Â 2   probs:Â 0Â 0Â 100Â 0Â 0Â 0Â 0Â 0Â 0Â 0
SampleÂ 2Â â†’Â 1   probs:Â 0Â 100Â 0Â 0Â 0Â 0Â 0Â 0Â 0Â 0
...
```

---

## ğŸ“Œ Roadmap

- [x] **Full int8 quantization support** (asymmetric zeroâ€‘pointsÂ + bias quantization)  
- [x] **Basic multithreaded inference** at the task level  
- [x] **Export weights from PyTorch to C headers**  
- [ ] **Improve Pythonâ€¯â†’â€¯C compilation path** (streamlined frontâ€‘endsÂ & codeâ€‘gen)  
- [ ] **Expand convolution kernels** (stride, dilation, padding variants)  
- [ ] **Optimize & multithread kernels at the RVV level  
- [ ] **Smarter tilingÂ /Â parallelism** inside convolution kernels  
- [ ] **Oneâ€‘click tooling** for `.pyÂ â†’Â .hÂ â†’Â .cÂ â†’Â .riscv` flow  
- [ ] **Better buildâ€‘time diagnostics** (shape checks, warnings, â€¦)

---

## ğŸ“« ContactÂ / Contributions

Earlyâ€‘stage project â€” **PRs and discussions welcome!** ğŸ™‚
